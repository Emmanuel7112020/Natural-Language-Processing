{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10560c4",
   "metadata": {},
   "source": [
    "### Setting Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# r means raw text i.e consider thus text as it is\n",
    "os.chdir(r\"D:\\ORBA Winter Semester 2020-2021\\Scientific Project\\Text Mining\\Recommended Materials\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef2225",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library needed to read the PDF file containing conference proceedings\n",
    "!pip install PyPDF2\n",
    "\n",
    "# Library needed for text pre-processing\n",
    "!pip install nltk\n",
    "\n",
    "# Libraries needed for generating word cloud (Exploratory Data Analysis of the pre-processed tokens)\n",
    "!pip install wordcloud\n",
    "!pip install matplotlib\n",
    "\n",
    "# Installing GenSim- a python-based open-source framework for unsupervised topic modeling and natural language processing\n",
    "!pip install gensim\n",
    "\n",
    "# Installing Regular Expressions (RegEx) library\n",
    "# RegEx is a sequence of characters that forms a search pattern. RegEx are used to match strings of text such\n",
    "#as particular characters, words, or patterns of characters.\n",
    "#!pip install regex\n",
    "\n",
    "# Installing the pyldavis library for visualization\n",
    "!pip install pyldavis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5531cb",
   "metadata": {},
   "source": [
    "### Importing Library Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d8052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing function to read PDF file contents\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "# Importing functions needed to handle regular expressions, tokenize text (sentence & word), remove stopwords,punctuations and lemmatize (Text Pre-processing)\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Importing POS tags function of NLTK\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Importing Functions for Generating Term frequency histogram andWord Cloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing functions needed for topic modelling\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Importing functions needed to perform model evaluation\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#Importing function for topic model visualization\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8a931",
   "metadata": {},
   "source": [
    "### Reading the PDF File and Extracting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6566da",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'INFORMS.pdf'\n",
    "pdfFileObj = open(filename,'rb')\n",
    "pdfReader = PdfFileReader(pdfFileObj,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e1a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of pages in the file and Initializing a count object\n",
    "num_pages = pdfReader.numPages \n",
    "print(num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "text = '''  '''\n",
    "\n",
    "# Creation of loop to read all pages of the PDF file\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4148cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7922205",
   "metadata": {},
   "source": [
    "### Tokenization of Text- Using NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7724d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Word Tokenization\n",
    "tokens= word_tokenize(text)\n",
    "type(tokens)\n",
    "len(tokens)\n",
    "print(tokens)\n",
    "\n",
    "# Alternative Way to Tokenize \n",
    "#tokens= text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03698832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Alphabets only from Tokens\n",
    "alphabetic_only = [word for word in tokens if word.isalpha()]\n",
    "print(alphabetic_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904138ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting alphabet_only tokens to lower case\n",
    "lower_case_tokens = [word.lower() for word in alphabetic_only]\n",
    "print(lower_case_tokens)\n",
    "print(len(lower_case_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225236f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Parts of Speech Tagging (POS-Tagging)\n",
    "POS_tags=pos_tag(tokens)\n",
    "print(POS_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9d2e2",
   "metadata": {},
   "source": [
    "### Removal of Stopwords, Numeric Characters and Punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "exclude = set(string.punctuation)\n",
    "print(exclude)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "# Extracting Tokens Without Stopwords\n",
    "tokens_without_stopwords = [word for word in lower_case_tokens if word not in stop]\n",
    "print(tokens_without_stopwords)\n",
    "print(len(tokens_without_stopwords))\n",
    "type(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a66ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing numeric characters ( Fun fact: use of \".extend\" and \".append\"; \".isdigit() and \".isnumeric())\n",
    "\n",
    "stopwords_num_free_tokens =[]\n",
    "        \n",
    "        \n",
    "stopwords_num_free_tokens.extend(str(j) for j in tokens_without_stopwords if not j.isnumeric())  \n",
    "print(stopwords_num_free_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a78d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations\n",
    "\n",
    "punc_free_tokens=[]\n",
    "punc_free_tokens.extend(k for k in stopwords_num_free_tokens if k not in exclude)\n",
    "\n",
    "print(punc_free_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b297a",
   "metadata": {},
   "source": [
    "### Lemmatization/Stemming of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization of Tokens\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "#lemma =lemma.join(lemmatizer.lemmatize(word) for word in punc_free_tokens)\n",
    "#lemma = ''' '''.join(lemmatizer.lemmatize(word) for word in punc_free_tokens.split())\n",
    "\n",
    "\n",
    "# Alternative way to Lemmatize\n",
    "lemma = [lemmatizer.lemmatize (t) for t in punc_free_tokens]\n",
    "\n",
    "print(lemma)\n",
    "print(len(lemma))\n",
    "type(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming of Tokens\n",
    "\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "#porter= PorterStemmer()\n",
    "#stemmed=[porter.stem(token) for token in lemma]\n",
    "#lemma=stemmed\n",
    "#print(lemma)\n",
    "#print(len(lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342306f",
   "metadata": {},
   "source": [
    "### Creating a gensim corpus Using the Lemmatized Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789023ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  splitting each element in lemma (list) to create an array needed by the dictionary \n",
    "#(without this an error \"doc2bow expects an array of unicode tokens on input, not a single string\") will be generated.\n",
    "lemma_array = [item.split() for item in lemma]\n",
    "\n",
    "\n",
    "# Alternative way to split strings in the lemma\n",
    "#lemma_array= [item.split('-') for item in lemma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dictionary of corpus(words); all unique terms are assigned an index\n",
    "dictionary = corpora.Dictionary(lemma_array)\n",
    "\n",
    "# The corpus = lemma_array\n",
    "\n",
    "# Converting tokenized and cleaned text into bag of words vectors/Generation of Document Term Matrix using the dictionary\n",
    "corpus = [dictionary.doc2bow(text) for text in lemma_array]\n",
    "\n",
    "# Printing corpus and dictionary\n",
    "# dictionary.token2id \n",
    "corpus\n",
    "#print(dictionary)\n",
    "\n",
    "for j in dictionary.values():\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d5961",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis of Pre-Processed Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Term Frequency Histogram\n",
    "%matplotlib inline\n",
    "\n",
    "word_freq= nltk.FreqDist(lemma)\n",
    "plt.hist(word_freq.values(), bins = range(50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b37d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_freq.hapaxes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b12a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of Word Cloud\n",
    "data_tokens=(str(lemma_array))  \n",
    "print(type(data_tokens))\n",
    "wordcloud = WordCloud(width = 800, height = 500, \n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate(data_tokens) \n",
    "\n",
    "plt.figure(figsize = (5, 5), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80808b57",
   "metadata": {},
   "source": [
    "### Topic Modelling Using Latent Dirichlet Allocation (LDA) with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFYING FEW MODEL PARAMETERS\n",
    "\n",
    "# Define a LDA model with 3 topics: Training the LDA model on document term matrix\n",
    "#ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=100)\n",
    "\n",
    "# Define a LDA model with 4 topics: Training the LDA model on document term matrix \n",
    "#ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word=dictionary, passes=100)\n",
    "\n",
    "# Define a LDA model with 5 topics: Training the LDA model on document term matrix \n",
    "#ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=100)\n",
    "\n",
    "# SPECIFYING MORE MODEL PARAMETERS\n",
    "\n",
    "# Define a LDA model with 3 topics: Training the LDA model on document term matrix\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=100, update_every=1, chunksize=100,passes=100, alpha='auto',per_word_topics=True)\n",
    "\n",
    "# Define a LDA model with 4 topics : Training the LDA model on document term matrix \n",
    "#ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=4, random_state=100, update_every=1, chunksize=100,passes=100, alpha='auto',per_word_topics=True)\n",
    "\n",
    "# Define a LDA model with 5 topics : Training the LDA model on document term matrix\n",
    "#ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=100, update_every=1, chunksize=100,passes=100, alpha='auto',per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf15fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the three topics from the model with weight of the top 20 keywords contributing to each topic\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "# Print the results\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae82263",
   "metadata": {},
   "source": [
    "### Evaluating The LDA Topic Model By Computing Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65acbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Perplexity : a measure of how good the model is. The lower the perplexity,the better the model is.\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  \n",
    "\n",
    "# Computing Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=lemma_array, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2327b3",
   "metadata": {},
   "source": [
    "### Determining Optimal Number of Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18298b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values = []\n",
    "model_list = []\n",
    "\n",
    "for i in range(1,11,1):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=dictionary, passes=20)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=lemma_array, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        #print(coherence_values)\n",
    "        #print(model_list)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a36f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=range(1,11,1)               \n",
    "plt.plot(x, coherence_values) \n",
    "plt.title('Plot of Coherence Scores Vs Topic Numbers')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Scores')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484622f",
   "metadata": {},
   "source": [
    "### Visualizing the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display= pyLDAvis.gensim_models.prepare(ldamodel,corpus,dictionary,sort_topics=False)\n",
    "\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23343011",
   "metadata": {},
   "source": [
    "### Assigning Topics To The Lemmatized Strings(Documents) - Maybe Not Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the Pandas library \n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6db564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Determining What topic a given text is about by finding the topic number with the highest percentage contribution in the text.\n",
    "\n",
    "def get_topic_details(ldamodel, corpus): \n",
    "    topic_details_df = pd.DataFrame() \n",
    "    for i, row in enumerate(ldamodel[corpus]): \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True) \n",
    "        for j, (topic_num, prop_topic) in enumerate(row): \n",
    "            if j == 0:  # => dominant topic \n",
    "                wp = ldamodel.show_topic(topic_num) \n",
    "                topic_details_df = topic_details_df.append(pd.Series([topic_num, prop_topic]),  \n",
    "                                                                      ignore_index=True) \n",
    "    topic_details_df.columns = ['Dominant_Topic', '% Score'] \n",
    "    return topic_details_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8bed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = pd.DataFrame({'Original text':lemma}) \n",
    "topic_details = pd.concat([get_topic_details(ldamodel, \n",
    "                           corpus), contents], axis=1) \n",
    "topic_details.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the topic associated with each document\n",
    "count = 0 \n",
    "for k in ldamodel[corpus]: \n",
    "    print(\"doc : \",count,k) \n",
    "    count. += 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
